{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train the convolutional neural network with hyperparameter tuning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we first do is filter the data into training, test and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Frank\\anaconda3\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mimport\u001b[39;00m read_image\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms\n",
      "File \u001b[1;32mc:\\Users\\Frank\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\__init__.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets, io, models, ops, transforms, utils\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mextension\u001b[39;00m \u001b[39mimport\u001b[39;00m _HAS_OPS\n\u001b[0;32m      9\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Frank\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\models\\__init__.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mswin_transformer\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmaxvit\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m detection, optical_flow, quantization, segmentation, video\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m \u001b[39mimport\u001b[39;00m get_model, get_model_builder, get_model_weights, get_weight, list_models\n",
      "File \u001b[1;32mc:\\Users\\Frank\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\models\\detection\\__init__.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmask_rcnn\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mretinanet\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mssd\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mssdlite\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:846\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:941\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1039\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import PIL\n",
    "import seaborn as sns\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from functools import partial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter the data into train/val/test in ratio 80|10|10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeling = pd.read_csv('./labels_upsampled.csv')\n",
    "labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = './DataFilteredPlural/'\n",
    "target_dir = './trainvaltest/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training via Convolutional Neural Network:\n",
    "But first we need to load the data into dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency of each class in train/val/test\n",
    "train_freq = {\"metal\": 0, \"plastic\": 0, \"wood,paper\": 0}\n",
    "val_freq = {\"metal\": 0, \"plastic\": 0, \"wood,paper\": 0}\n",
    "test_freq = {\"metal\": 0, \"plastic\": 0, \"wood,paper\": 0}\n",
    "\n",
    "for file in os.listdir(target_dir + \"train\"):\n",
    "    material_type = labeling[labeling['filename'] == file]['label'].values[0]\n",
    "    train_freq[material_type] += 1\n",
    "for file in os.listdir(target_dir + \"val\"):\n",
    "    material_type = labeling[labeling['filename'] == file]['label'].values[0]\n",
    "    val_freq[material_type] += 1\n",
    "for file in os.listdir(target_dir + \"test\"):\n",
    "    material_type = labeling[labeling['filename'] == file]['label'].values[0]\n",
    "    test_freq[material_type] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot frequency of each class in train/val/test\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(train_freq.keys(), train_freq.values())\n",
    "plt.title(\"Train\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(val_freq.keys(), val_freq.values())\n",
    "plt.title(\"Val\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(test_freq.keys(), test_freq.values())\n",
    "plt.title(\"Test\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the factor to balance the dataset\n",
    "ratio = 1/3\n",
    "metal_factor = 1 / (train_freq[\"metal\"]) * len(os.listdir(target_dir + \"train\")) * ratio\n",
    "plastic_factor = 1 / (train_freq[\"plastic\"]) * len(os.listdir(target_dir + \"train\")) * ratio\n",
    "wood_paper_factor = 1 / (train_freq[\"wood,paper\"]) * len(os.listdir(target_dir + \"train\")) * ratio\n",
    "print(\"metal_factor: \", metal_factor)\n",
    "print(\"plastic_factor: \", plastic_factor)\n",
    "print(\"wood_paper_factor: \", wood_paper_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom dataset class\n",
    "class Top4Dataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, label, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.image_names = os.listdir(root_dir)\n",
    "        self.label = label\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    \n",
    "    def __items_class__(self):\n",
    "        return self.label\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.image_names[index]\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        image = read_image(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        image = image.float()\n",
    "        material_name = self.annotations[self.annotations[\"filename\"] == img_name][\"label\"].values[0]\n",
    "        y_label = [0.0, 0.0, 0.0]\n",
    "        y_label[self.label[material_name]] = 1.0\n",
    "        y_label = torch.tensor(y_label)\n",
    "        path = img_name\n",
    "        return (image, y_label, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "material_label = {\"metal\": 0, \"plastic\": 1, \"wood,paper\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data transforms\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((144,144)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_label = \"C:/Users/Frank/Documents/GitHub/AI_V2/convnetupsample_Plural/labels_upsampled.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top4Dataset_train = Top4Dataset(csv_file=csv_file_label, root_dir=target_dir + \"train/\", label=material_label, transform=data_transforms)\n",
    "Top4Dataset_val = Top4Dataset(csv_file=csv_file_label, root_dir=target_dir + \"val/\", label=material_label, transform=data_transforms)\n",
    "Top4Dataset_test = Top4Dataset(csv_file=csv_file_label, root_dir=target_dir + \"test/\", label=material_label, transform=data_transforms)\n",
    "\n",
    "print(f\"length of train dataset: {len(Top4Dataset_train)}\")\n",
    "print(f\"length of val dataset: {len(Top4Dataset_val)}\")\n",
    "print(f\"length of test dataset: {len(Top4Dataset_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    train_dataset = Top4Dataset(csv_file=csv_file_label, root_dir=data_dir + \"train/\", label=material_label, transform=data_transforms)\n",
    "    val_dataset = Top4Dataset(csv_file=csv_file_label, root_dir=data_dir + \"val/\", label=material_label, transform=data_transforms)\n",
    "    test_dataset = Top4Dataset(csv_file=csv_file_label, root_dir=data_dir + \"test/\", label=material_label, transform=data_transforms)\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_count = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader_train = DataLoader(dataset=Top4Dataset_train, batch_size=batch_count, shuffle=True)\n",
    "DataLoader_val = DataLoader(dataset=Top4Dataset_val, batch_size=batch_count, shuffle=True)\n",
    "DataLoader_test = DataLoader(dataset=Top4Dataset_test, batch_size=batch_count, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['metal', 'plastic', 'wood,paper']\n",
    "DataLoader_train.dataset.__items_class__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show images\n",
    "data = next(iter(DataLoader_train))\n",
    "images, labels, path = data\n",
    "plt.figure(figsize=(10, 15))\n",
    "for i in range(batch_count):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(images[i].permute(1, 2, 0))\n",
    "    plt.title(label_list[labels[i].argmax()])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class ConvNeuralNetwork(nn.Module):\n",
    "    def __init__(self, c1_out=16, c2_out=25, c1_kernel=3, c2_kernel=3, c1_stride=1, c2_stride=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv_relu_stack = nn.Sequential(\n",
    "            nn.Conv2d(3, c1_out, kernel_size=c1_kernel, stride=c1_stride, padding= 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size= 2, stride=2),\n",
    "            nn.Conv2d(c1_out, c2_out, kernel_size=c2_kernel, stride=c2_stride, padding= 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size= 2, stride=2),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        s = self.flatten_counter()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(s, 3),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = nn.functional.normalize(x)\n",
    "        logits = self.conv_relu_stack(x)\n",
    "        logits = self.flatten(logits)\n",
    "        logits = self.linear_relu_stack(logits)\n",
    "        return logits\n",
    "\n",
    "    def flatten_counter(self):\n",
    "        x = torch.rand(1, 3, 144, 144)\n",
    "        logits = self.conv_relu_stack(x)\n",
    "        logits = self.flatten(logits)\n",
    "        return logits.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convolutionlayer number of parameters\n",
    "input_size = (3, 144, 144)\n",
    "in_channels = 3\n",
    "out_channels = 16\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "number_of_parameters = (in_channels * out_channels * kernel_size * kernel_size) + out_channels\n",
    "test = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "print(f\"number of parameters: {number_of_parameters}\")\n",
    "print(f\"number of parameters: {test.weight.numel() + test.bias.numel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for (X, y, z) in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cifar(config, checkpoint_dir = None, data_dir = None):\n",
    "    net = ConvNeuralNetwork(c1_out=config[\"c1_out\"], c2_out=config[\"c2_out\"], c1_kernel=config[\"c1_kernel\"], c2_kernel=config[\"c2_kernel\"], c1_stride=config[\"c1_stride\"], c2_stride=config[\"c2_stride\"], dropout=config[\"dropout\"])\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "    net.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
    "\n",
    "    #trail name\n",
    "    trail_name = f\"trail_{config['c1_out']}_{config['c2_out']}_{config['c1_kernel']}_{config['c2_kernel']}_{config['c1_stride']}_{config['c2_stride']}_{config['dropout']}_{config['lr']}_{config['momentum']}\"\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = load_data(data_dir)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    #chart data\n",
    "    char_df = pd.DataFrame(columns=[\"epoch\", \"train_loss\", \"test_loss\"])\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels, path = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels.argmax(dim=1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 50 == 49:    # print every 50 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 50))\n",
    "                running_loss = 0.0\n",
    "        avg_testloss = test(test_loader, net, criterion)\n",
    "        char_df = char_df.append({\"epoch\": epoch, \"train_loss\": loss.item(), \"test_loss\": avg_testloss}, ignore_index=True)\n",
    "        print(\"Saving checkpoint\")\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "        char_name = f\"char_data_{trail_name}.csv\"\n",
    "        char_df.to_csv(\"C:/Users/Frank/Documents/GitHub/AI_V2/convnetupsample_Plural/checkpoint/\" + char_name, index=False)  \n",
    "        tune.report(loss=avg_testloss, accuracy=avg_testloss, epoch=epoch, checkpoint=path, config=config)  \n",
    "    print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [metal_factor, plastic_factor, wood_paper_factor]\n",
    "DataLoader_train.dataset.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"C:/Users/Frank/Documents/GitHub/AI_V2/convnetupsample_Plural/trainvaltest/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"momentum\": tune.uniform(0.1, 0.9),\n",
    "    \"batch_size\": tune.choice([4, 8, 16, 32, 64]),\n",
    "    \"epochs\": 50,\n",
    "    \"c1_out\": tune.choice([16, 32, 64]),\n",
    "    \"c2_out\": tune.choice([16, 32, 64]),\n",
    "    \"c1_kernel\": tune.choice([3, 5, 7]),\n",
    "    \"c2_kernel\": tune.choice([3, 5, 7]),\n",
    "    \"c1_stride\": tune.choice([1, 2, 3]),\n",
    "    \"c2_stride\": tune.choice([1, 2, 3]),\n",
    "    \"c1_pool_kernel\": tune.choice([2, 3, 4]),\n",
    "    \"c2_pool_kernel\": tune.choice([2, 3, 4]),\n",
    "    \"dropout\": tune.uniform(0.1, 0.5),\n",
    "}\n",
    "train_dataset, val_dataset, test_dataset = load_data(data_dir)\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    grace_period=1,\n",
    "    reduction_factor=2)\n",
    "reporter = CLIReporter(\n",
    "    parameter_columns=[\"c1_out\", \"c2_out\", \"c1_kernel\", \"c2_kernel\", \"c1_stride\", \"c2_stride\", \"c1_pool_kernel\", \"c2_pool_kernel\", \"dropout\", \"lr\", \"momentum\", \"batch_size\", \"epochs\"],\n",
    "    metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "result = tune.run(\n",
    "    partial(train_cifar, data_dir=data_dir, checkpoint_dir=True),\n",
    "    resources_per_trial={\"cpu\": 6, \"gpu\": 1},\n",
    "    config=config,\n",
    "    num_samples= 10,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter)\n",
    "\n",
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "print(\"Best trial config: {}\".format(best_trial.config))\n",
    "print(\"Best trial final validation loss: {}\".format(\n",
    "    best_trial.last_result[\"loss\"]))\n",
    "print(\"Best trial final validation accuracy: {}\".format(\n",
    "    best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "best_trained_model = ConvNeuralNetwork(c1_out=best_trial.config[\"c1_out\"], c2_out=best_trial.config[\"c2_out\"], c1_kernel=best_trial.config[\"c1_kernel\"], c2_kernel=best_trial.config[\"c2_kernel\"], c1_stride=best_trial.config[\"c1_stride\"], c2_stride=best_trial.config[\"c2_stride\"], dropout=best_trial.config[\"dropout\"])\n",
    "best_trained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial.checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a71982e2765f6eb6b33820c8a9a4dd21ae31a9aa60cef222f62c4fbc4323d02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
